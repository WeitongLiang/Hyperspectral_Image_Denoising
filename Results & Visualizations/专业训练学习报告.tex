\documentclass{article}[UTF8]
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{tikz,pgfplots}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx} 
\usepackage{float} 
\usepackage{subfigure} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\title{Stat 210B Homework1}                                                                                                             
\author{Weitong Liang}
\date{}
\newtheorem{Prop}{\emph{Prop}}
\newcommand{\EE}{\EE}
\newcommand{\PP}{\PP}

\begin{document}

\lstset{language=matlab}

\lstset{
    numbers=left, 
    numberstyle= \tiny, 
    keywordstyle= \color{ blue!70},
    commentstyle= \color{red!50!green!50!blue!50}, 
    frame=shadowbox, 
    rulesepcolor= \color{ red!20!green!20!blue!20} ,
    escapeinside=``, 
    xleftmargin=2em, aboveskip=1em,
    framexleftmargin=2em
}
\maketitle
\section{BIRCH 聚类分析}
由于每个人对于单词的熟悉程度存在差异,并且许多单词的属性对于Wordle游戏的难度影响是双向的,因此在衡量单词难度时
直接利用上文中所提到的单词诸多特性是无法完成的.根据我们的假设,在不同日期整个玩家群里的水平是相同的,那么某天单
词的难度就完全体现在当天的游戏结果中,即用户用不同轮数完成游戏的百分比.对于所给数据中的单词,我们用向量$R=(r_{1},
\dots,r_{7})$来表示数据集中的一个单词,其中$r_{i}$表示用$i$轮完成游戏的玩家的百分比(i=7时为超过七轮完成游戏的玩家百分比).
若$R$和$W$表示两个难度相近的词,则$R$和$W$在某种距离意义下是靠近的.我们利用$BIRCH$聚类算法来对单词的难度进行划分.

BIRCH算法利用树结构进行快速聚类,即在读入数据的同时构建并更新CF-tree(Clustering Feature Tree),利用节点对于数据进行划分.
首先我们定义一个簇的clustering feature:
\[
    CF=(N,\overrightarrow{LS},SS )
    \]
其中$N$表示簇中点的数量,$\overrightarrow{LS}$是每个点的线性求和,即设$R_{i},i\in [N]$是一个簇中的全体点,则有：
\[
    \overrightarrow{LS}=\sum_{i=1}^{N}R_{i}=(\sum_{i=1}^{N}r_{i1},\dots,\sum_{i=1}^{N}r_{i7}).
    \]
$SS$是每点所包含数据的平方和,即:
\[
    SS=\sum_{i=1}^{N}R_{i}^{T}R_{i}.
    \]
对于两个不相交的簇$C_{1},C_{2}$,其clustering feature 分别为$CF_{1}$,$CF_{2}$.如果将这两个簇合并,
则合并之后的簇的clustering feature为:
\[
    CF_{1}+CF_{2}=(N_{1}+N_{2},\overrightarrow{LS_{1}}+\overrightarrow{LS_{2}},SS_{1}+SS_{2})
    \]
有多种度量合理作为衡量簇之间差异的标准,这里我们选用簇联通平均距离$d$,有:
\[
    d=\sqrt{\frac{\sum_{m=1}^{N_{1}\sum_{n=1}^{N_{2}}(r_{1m}-r_{2n})^2}}{N_{1}N_{2}}}.
    \]
对于一个簇,其直径$D$定义为：
\[\sqrt{\frac{2NSS-2LS^{T}LS}{N(N-1)}}
    \]

和一般的树形结构类似,CF-tree由根节点,叶节点,枝节点组成。在构造CF-tree之前,我们给出以下参数.枝平衡因子$c1$表示枝节点包含的最大元素数量；
叶平衡因子$c2$表示叶子节点允许包含的最大$CF$数,空间阈值$c3$表示叶子节点每个$CF$的最大直径.

构造CF-tree并聚类的过程如下.在没有添加样本之前CF-tree是空的.读入第一个单词$R_{1}$后构造一个叶子节点和一个子簇.读入第二个单词$R_{2}$,
判断$R_{1}$与$R_{2}$是否在一个直径为$c3$的超球体内.若在,则$R_{1},R_{2}$属于同一个CF,根据$R_{2}$的数据和$CF$的可加性更新$CF$. 若不在,
则建立一个新的分支.依次类推,直至分支数量达到$c1$.此时若新的单词$R_{i}$不属于任何一个已经建立的节点,则选取与其距离最近的$R_{j}$所在的节点,
由其再衍生出两个子节点.这一级别的子节点的数量收$c_{2}$约束.由此不断添加样本,我们就能在CF-tree建立的同时实现依照难度的分类.

\end{proof}
\end{document}